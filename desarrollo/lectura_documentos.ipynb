{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ccf86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re, unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646d094",
   "metadata": {},
   "source": [
    "**Definimos la ruta en donde se encuentran las carpetas con los diferenetes temas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1338018",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path.cwd()\n",
    "base_atras_carpetas = base.parent #las carpetas se encuentran en la carpeta que contiene a la carpeta del archivo actual\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a8089",
   "metadata": {},
   "source": [
    "**Función para normalizar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117abe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto: str):\n",
    "    t = texto.lower()\n",
    "\n",
    "    t = t.replace(\"ñ\", \"<<enye>>\")\n",
    "\n",
    "    t = unicodedata.normalize(\"NFD\", t)\n",
    "\n",
    "    t = \"\".join(ch for ch in t if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "    t = t.replace(\"<<enye>>\", \"ñ\")\n",
    "\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t, flags=re.UNICODE)\n",
    "    \n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    \n",
    "    return t.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750186d",
   "metadata": {},
   "source": [
    "**Recorremos las carpetas recursivamente y leemos los archivos txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_docs(extension=\".txt\"):\n",
    "    \n",
    "    carpeta = defaultdict(lambda:defaultdict(list))\n",
    "\n",
    "    for car in base_atras_carpetas.iterdir(): \n",
    "        if car.is_dir():\n",
    "            nombre_carpeta = car.name\n",
    "            for c in car.rglob(f\"*{extension}\"):\n",
    "                try:\n",
    "                    texto = c.read_text(encoding=\"utf-8\")\n",
    "                except UnicodeDecodeError:\n",
    "                    texto = c.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "                texto = normalizar_texto(texto)\n",
    "                nombre_archivo = c.name\n",
    "                carpeta[nombre_carpeta][nombre_archivo].append(texto)\n",
    "\n",
    "    return carpeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68caa071",
   "metadata": {},
   "source": [
    "**set que contiene las stopwords, al ser set es facil de buscar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b580ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_ES = {\n",
    "    \"de\",\"la\",\"que\",\"el\",\"en\",\"y\",\"a\",\"los\",\"del\",\"se\",\"las\",\"por\",\"un\",\"para\",\"con\",\n",
    "    \"no\",\"una\",\"su\",\"al\",\"lo\",\"como\",\"más\",\"pero\",\"sus\",\"le\",\"ya\",\"o\",\"este\",\"sí\",\n",
    "    \"porque\",\"esta\",\"entre\",\"cuando\",\"muy\",\"sin\",\"sobre\",\"también\",\"me\",\"hasta\",\"hay\",\n",
    "    \"donde\",\"quien\",\"desde\",\"todo\",\"nos\",\"durante\",\"todos\",\"uno\",\"les\",\"ni\",\"contra\",\n",
    "    \"otros\",\"ese\",\"eso\",\"ante\",\"ellos\",\"e\",\"esto\",\"mí\",\"antes\",\"algunos\",\"qué\",\"unos\",\n",
    "    \"yo\",\"otro\",\"otras\",\"otra\",\"él\",\"tanto\",\"esa\",\"estos\",\"mucho\",\"quienes\",\"nada\",\n",
    "    \"muchos\",\"cual\",\"poco\",\"ella\",\"estar\",\"estas\",\"algunas\",\"algo\",\"nosotros\",\"mi\",\n",
    "    \"mis\",\"tú\",\"te\",\"ti\",\"tu\",\"tus\",\"ellas\",\"nosotras\",\"vosostros\",\"vosostras\",\"os\",\n",
    "    \"mío\",\"mía\",\"míos\",\"mías\",\"tuyo\",\"tuya\",\"tuyos\",\"tuyas\",\"suyo\",\"suya\",\"suyos\",\n",
    "    \"suyas\",\"nuestro\",\"nuestra\",\"nuestros\",\"nuestras\",\"vuestro\",\"vuestra\",\"vuestros\",\n",
    "    \"vuestras\",\"esos\",\"esas\",\"estoy\",\"estás\",\"está\",\"estamos\",\"estáis\",\"están\",\"esté\",\n",
    "    \"estés\",\"estemos\",\"estéis\",\"estén\",\"estaré\",\"estarás\",\"estará\",\"estaremos\",\n",
    "    \"estaréis\",\"estarán\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21c849",
   "metadata": {},
   "source": [
    "**funcion get_Tokens que recorre la lista str que corresponde a cada txt y extraer las palabras que no sean stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0720df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(texto: str):\n",
    "    palabras_unicas = []\n",
    "    \n",
    "    for pal in texto:\n",
    "        if pal not in palabras_unicas and pal not in STOPWORDS_ES:\n",
    "            palabras_unicas.append(pal)\n",
    "    \n",
    "    return palabras_unicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002279c",
   "metadata": {},
   "source": [
    "**función preparacion_datos, se le pasa el corpus y esta devuelve otro corpus pero esta vez con palabras unicas y sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f300c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    for etiqueta, texto in corpus.items():\\n        for text in texto:\\n\\n            tokens = get_Tokens(text)\\n            palabras_por_etiqueta[etiqueta].append(tokens)\\n\\n    return palabras_por_etiqueta\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preparacion_datos(corpus: dict[str, dict[str, list[str]]]):\n",
    "    \n",
    "    corpus_actualizado = defaultdict(lambda:defaultdict(list))\n",
    "    \n",
    "    for carpeta, archivo_texto in corpus.items():\n",
    "        for nombre_arch_texto, lista_texto in archivo_texto.items():\n",
    "            tokens = get_tokens(lista_texto[0])\n",
    "            corpus_actualizado[carpeta][nombre_arch_texto].append(tokens)\n",
    "    return corpus_actualizado\n",
    "'''\n",
    "    for etiqueta, texto in corpus.items():\n",
    "        for text in texto:\n",
    "            \n",
    "            tokens = get_Tokens(text)\n",
    "            palabras_por_etiqueta[etiqueta].append(tokens)\n",
    "\n",
    "    return palabras_por_etiqueta\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2269fde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpus = leer_docs()\\ncorpus_actualizado = preparacion_datos(corpus)\\nfor etiqueta, valor in corpus_actualizado.items():\\n    print(etiqueta)\\n    for texto_nombre, lista_texto in valor.items():\\n        print(texto_nombre)\\n        print(lista_texto)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#código para ver como queda el corpus una vez aplicada la función preparación_datos\n",
    "'''\n",
    "corpus = leer_docs()\n",
    "corpus_actualizado = preparacion_datos(corpus)\n",
    "for etiqueta, valor in corpus_actualizado.items():\n",
    "    print(etiqueta)\n",
    "    for texto_nombre, lista_texto in valor.items():\n",
    "        print(texto_nombre)\n",
    "        print(lista_texto)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec4338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (laboratorio1)",
   "language": "python",
   "name": "laboratorio1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
